{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23978773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import functions as func\n",
    "\n",
    "file = \"Jan-2017 onwards.csv\"\n",
    "original_df = pd.read_csv(file)\n",
    "\n",
    "filter_col = [\"month\", \"town\", \"flat_type\", \"storey_range\", \"floor_area_sqm\", \"remaining_lease\", \"resale_price\"]\n",
    "col_filtered_df = original_df[filter_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b00bc692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wenyu\\AppData\\Local\\Temp\\ipykernel_21572\\1226857134.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  col_filtered_df.loc[:, 'lease_months'] = (years * 12) + months\n"
     ]
    }
   ],
   "source": [
    "extracted_data = col_filtered_df['remaining_lease'].str.extract(\n",
    "    r'(?P<years>\\d+) years(?: (?P<months>\\d+) months)?'\n",
    ")\n",
    "\n",
    "# Use .fillna(0) for any rows that might not match the pattern (coerced to NaN).\n",
    "years = pd.to_numeric(extracted_data['years'], errors='coerce').fillna(0).astype(int)\n",
    "months = pd.to_numeric(extracted_data['months'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "col_filtered_df.loc[:, 'lease_months'] = (years * 12) + months\n",
    "col_filtered_df = col_filtered_df.drop(columns=\"remaining_lease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcc11ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "amk_df = col_filtered_df[col_filtered_df[\"town\"] == \"ANG MO KIO\"]\n",
    "clem_df = col_filtered_df[col_filtered_df[\"town\"] == \"CLEMENTI\"]\n",
    "bb_df = col_filtered_df[col_filtered_df[\"town\"] == \"BUKIT BATOK\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265be0d6",
   "metadata": {},
   "source": [
    "#####################################################\n",
    "#2D Linear Regression Model#\n",
    "Resale price over time\n",
    "Categories: \n",
    "lease_months: 50-59yrs(600-719mths), 60-69yrs(720-839mths), 70-79yrs(840-959mths), 80-89yrs(960-1079mths), 90-99yrs(1080-1188mths) \n",
    "storey_range: 01 TO 03, 04 TO 06, 07 TO 09, 10 TO 12\n",
    "flat_type: 3 ROOM, 4 ROOM, 5 ROOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e478ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "amk_1to3_50yrs_3rm = amk_df[(amk_df[\"lease_months\"] >= 600) & \n",
    "                        (amk_df[\"lease_months\"] < 720) & \n",
    "                        (amk_df[\"storey_range\"] == \"01 TO 03\") &\n",
    "                        (amk_df[\"flat_type\"] == \"3 ROOM\")]\n",
    "\n",
    "amk_1to3_50yrs_4rm = amk_df[(amk_df[\"lease_months\"] >= 600) & \n",
    "                        (amk_df[\"lease_months\"] < 720) & \n",
    "                        (amk_df[\"storey_range\"] == \"01 TO 03\") &\n",
    "                        (amk_df[\"flat_type\"] == \"4 ROOM\")]\n",
    "\n",
    "amk_1to3_50yrs_5rm = amk_df[(amk_df[\"lease_months\"] >= 600) & \n",
    "                        (amk_df[\"lease_months\"] < 720) & \n",
    "                        (amk_df[\"storey_range\"] == \"01 TO 03\") &\n",
    "                        (amk_df[\"flat_type\"] == \"5 ROOM\")]\n",
    "\n",
    "all_4to6_50yrs_3rm = col_filtered_df[(col_filtered_df[\"lease_months\"] >= 600) & \n",
    "                                     (col_filtered_df[\"lease_months\"] < 720) & \n",
    "                                     (col_filtered_df[\"storey_range\"] == \"04 TO 06\") &\n",
    "                                     (col_filtered_df[\"flat_type\"] == \"3 ROOM\")]\n",
    "# all_4to6_50yrs_3rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a36bd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter outliers\n",
    "q1 = all_4to6_50yrs_3rm['resale_price'].quantile(0.25)\n",
    "q3 = all_4to6_50yrs_3rm['resale_price'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "all_4to6_50yrs_3rm = all_4to6_50yrs_3rm[(all_4to6_50yrs_3rm['resale_price'] > lower_bound) & (all_4to6_50yrs_3rm['resale_price'] < upper_bound)]\n",
    "# all_4to6_50yrs_3rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9f4beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "all_4to6_50yrs_3rm['month'] = pd.to_datetime(all_4to6_50yrs_3rm['month'])\n",
    "\n",
    "# 2. Filter using the year property\n",
    "# Part 1: 2017 to 2021\n",
    "training_data_df = all_4to6_50yrs_3rm[(all_4to6_50yrs_3rm['month'].dt.year >= 2017) & (all_4to6_50yrs_3rm['month'].dt.year <= 2021)]\n",
    "\n",
    "# Part 2: 2022 to 2025\n",
    "testing_data_df = all_4to6_50yrs_3rm[(all_4to6_50yrs_3rm['month'].dt.year >= 2022) & (all_4to6_50yrs_3rm['month'].dt.year <= 2025)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineer training month column (with normalisation to prevent Exploding Gradient)\n",
    "start_date = training_data_df['month'].min()\n",
    "training_data_df['days_passed'] = (training_data_df['month'] - start_date).dt.days\n",
    "min_date = training_data_df['days_passed'].min()\n",
    "max_date = training_data_df['days_passed'].max()\n",
    "training_data_df['days_passed_scaled'] = (training_data_df['days_passed'] - min_date) / max_date - min_date\n",
    "# training_data_df\n",
    "\n",
    "#Feature engineer testing month column\n",
    "start_date = testing_data_df['month'].min()\n",
    "testing_data_df['days_passed'] = (testing_data_df['month'] - start_date).dt.days\n",
    "min_date = testing_data_df['days_passed'].min()\n",
    "max_date = testing_data_df['days_passed'].max()\n",
    "testing_data_df['days_passed_scaled'] = (testing_data_df['days_passed'] - min_date) / max_date - min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert training df to np.array\n",
    "feature = [\"days_passed_scaled\", \"resale_price\"]\n",
    "training_data_df = training_data_df[feature]\n",
    "training_data_np = pd.DataFrame(training_data_df).to_numpy()\n",
    "# training_data_np\n",
    "\n",
    "# Convert testing df to np.array\n",
    "features = ['days_passed_scaled', 'resale_price']\n",
    "testing_data_np = pd.DataFrame(testing_data_df[features]).to_numpy()\n",
    "# testing_data_np\n",
    "# for i in testing_data_np:\n",
    "#     test_x, test_y = i\n",
    "#     print(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "993e5551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Cost: 90663605053.03063965 Loss_gradient: [-329268.39659753 -592921.9924465 ] Weight: [32926.83965975 59292.19924465]\n",
      "epoch: 10 Cost: 3611339444.01099586 Loss_gradient: [ -4380.80657637 -34021.35220591] Weight: [111629.4128598  221931.66105024]\n",
      "epoch: 20 Cost: 3263555208.90585566 Loss_gradient: [ 9433.64656593 -7223.2867149 ] Weight: [105078.78205551 235483.04043388]\n",
      "epoch: 30 Cost: 3139716560.64797115 Loss_gradient: [ 8919.32405397 -5297.94909027] Weight: [ 95784.98943374 241373.34547191]\n",
      "epoch: 40 Cost: 3044157251.58040953 Loss_gradient: [ 7876.76202218 -4612.30717491] Weight: [ 87443.16476341 246275.13880287]\n",
      "epoch: 50 Cost: 2970176844.95205069 Loss_gradient: [ 6931.95418177 -4055.98683363] Weight: [ 80095.69707883 250575.04781791]\n",
      "epoch: 60 Cost: 2912901932.88216352 Loss_gradient: [ 6099.35622037 -3568.6783018 ] Weight: [ 73630.4444498  254357.84602683]\n",
      "epoch: 70 Cost: 2868560251.25533724 Loss_gradient: [ 5366.70962761 -3140.00671147] Weight: [ 67941.77588689 257686.22933962]\n",
      "epoch: 80 Cost: 2834231351.24343109 Loss_gradient: [ 4722.0651478  -2762.83153289] Weight: [ 62936.42460071 260614.80874608]\n",
      "epoch: 90 Cost: 2807654249.68142128 Loss_gradient: [ 4154.85468771 -2430.96253823] Weight: [ 58532.31183595 263191.60961292]\n",
      "epoch: 100 Cost: 2787078515.32369137 Loss_gradient: [ 3655.77706169 -2138.95737643] Weight: [ 54657.21736495 265458.88733907]\n",
      "epoch: 110 Cost: 2771148981.27076530 Loss_gradient: [ 3216.64821715 -1882.02762785] Weight: [ 51247.59595989 267453.8215848 ]\n",
      "epoch: 120 Cost: 2758816490.44776869 Loss_gradient: [ 2830.26715751 -1655.96006309] Weight: [ 48247.53539037 269209.12601845]\n",
      "epoch: 130 Cost: 2749268795.61918545 Loss_gradient: [ 2490.29786352 -1457.04754276] Weight: [ 45607.83955569 270753.58477011]\n",
      "epoch: 140 Cost: 2741877062.85012722 Loss_gradient: [ 2191.1653932 -1282.0282259] Weight: [ 43285.2217491  272112.52444453]\n",
      "epoch: 150 Cost: 2736154455.00501966 Loss_gradient: [ 1927.96446188 -1128.03208116] Weight: [ 41241.59482639 273308.22943615]\n",
      "epoch: 160 Cost: 2731724067.49343586 Loss_gradient: [1696.37900352 -992.53382291] Weight: [ 39443.44663888 274360.30735683]\n",
      "epoch: 170 Cost: 2728294104.43567181 Loss_gradient: [1492.6113943  -873.31150069] Weight: [ 37861.29048879 275286.01056869]\n",
      "epoch: 180 Cost: 2725638660.29069662 Loss_gradient: [1313.32017771 -768.41006284] Weight: [ 36469.18159552 276100.51909472]\n",
      "epoch: 190 Cost: 2723582841.17884016 Loss_gradient: [1155.56527023 -676.10929686] Weight: [ 35244.29164351 276817.18954642]\n",
      "epoch: 200 Cost: 2721991245.95412016 Loss_gradient: [1016.75974862 -594.89562072] Weight: [ 34166.53443518 277447.77415041]\n",
      "epoch: 210 Cost: 2720759048.36204958 Loss_gradient: [ 894.62742872 -523.43726258] Weight: [ 33218.23650995 278002.61346582]\n",
      "epoch: 220 Cost: 2719805092.93272305 Loss_gradient: [ 787.16553965 -460.5624219 ] Weight: [ 32383.84732838 278490.80595255]\n",
      "epoch: 230 Cost: 2719066549.88818121 Loss_gradient: [ 692.61188168 -405.24005384] Weight: [ 31649.68426865 278920.35717124]\n",
      "epoch: 240 Cost: 2718494777.02345562 Loss_gradient: [ 609.41592903 -356.56296177] Weight: [ 31003.70825389 279298.31106132]\n",
      "epoch: 250 Cost: 2718052116.01505184 Loss_gradient: [ 536.21340375 -313.73292078] Weight: [ 30435.32633094 279630.86545022]\n",
      "epoch: 260 Cost: 2717709412.15407085 Loss_gradient: [ 471.80390382 -276.04758804] Weight: [ 29935.21796321 279923.47368757]\n",
      "epoch: 270 Cost: 2717444094.14748526 Loss_gradient: [ 415.13121846 -242.88898555] Weight: [ 29495.18218907 280180.93407126]\n",
      "epoch: 280 Cost: 2717238687.51911926 Loss_gradient: [ 365.26600808 -213.71336631] Weight: [ 29108.00313944 280407.4685317 ]\n",
      "epoch: 290 Cost: 2717079663.69889355 Loss_gradient: [ 321.39056453 -188.04229774] Weight: [ 28767.33170936 280606.79186461]\n",
      "epoch: 300 Cost: 2716956548.99906015 Loss_gradient: [ 282.78540211 -165.45481619] Weight: [ 28467.58144294 280782.17264758]\n",
      "epoch: 310 Cost: 2716861234.79207897 Loss_gradient: [ 248.8174591 -145.5805238] Weight: [ 28203.83692466 280936.48683943]\n",
      "epoch: 320 Cost: 2716787443.45712376 Loss_gradient: [ 218.92971664 -128.09351457] Weight: [ 27971.77317455 281072.26494125]\n",
      "epoch: 330 Cost: 2716730314.92103100 Loss_gradient: [ 192.63206449 -112.70703008] Weight: [ 27767.58472556 281191.73349246]\n",
      "epoch: 340 Cost: 2716686086.56225777 Loss_gradient: [169.49326404 -99.16875708] Weight: [ 27587.92322014 281296.85158248]\n",
      "epoch: 350 Cost: 2716651845.39570713 Loss_gradient: [149.13387671 -87.25668997] Weight: [ 27429.8425026  281389.34297656]\n",
      "epoch: 360 Cost: 2716625336.21653366 Loss_gradient: [131.22004174 -76.77549027] Weight: [ 27290.75030696 281470.72438271]\n",
      "epoch: 370 Cost: 2716604813.06703091 Loss_gradient: [115.45800146 -67.5532834 ] Weight: [ 27168.36574807 281542.33032327]\n",
      "epoch: 380 Cost: 2716588924.24367094 Loss_gradient: [101.58928411 -59.43884021] Weight: [ 27060.68191875 281605.3350189 ]\n",
      "epoch: 390 Cost: 2716576623.27066660 Loss_gradient: [ 89.38646534 -52.29909706] Weight: [ 26965.9329798  281660.77164385]\n",
      "epoch: 400 Cost: 2716567099.97662687 Loss_gradient: [ 78.64943882 -46.01697381] Weight: [ 26882.56520316 281709.54926836]\n",
      "epoch: 410 Cost: 2716559727.13470840 Loss_gradient: [ 69.20213484 -40.48945389] Weight: [ 26809.21149324 281752.46776592]\n",
      "epoch: 420 Cost: 2716554019.15197420 Loss_gradient: [ 60.88963301 -35.62589498] Weight: [ 26744.66896884 281790.2309299 ]\n",
      "epoch: 430 Cost: 2716549600.08709812 Loss_gradient: [ 53.57562186 -31.34654265] Weight: [ 26687.8792378  281823.45801461]\n",
      "epoch: 440 Cost: 2716546178.88990545 Loss_gradient: [ 47.14016354 -27.58122249] Weight: [ 26637.91104116 281852.69389005]\n",
      "epoch: 450 Cost: 2716543530.23220587 Loss_gradient: [ 41.47772702 -24.26818941] Weight: [ 26593.94498198 281878.41797688]\n",
      "epoch: 460 Cost: 2716541479.66710567 Loss_gradient: [ 36.49545757 -21.35311505] Weight: [ 26555.2600886  281901.05210819]\n",
      "epoch: 470 Cost: 2716539892.13946724 Loss_gradient: [ 32.11165412 -18.78819695] Weight: [ 26521.22199185 281920.96744682]\n",
      "epoch: 480 Cost: 2716538663.09096098 Loss_gradient: [ 28.25442942 -16.53137465] Weight: [ 26491.27252245 281938.49057184]\n",
      "epoch: 490 Cost: 2716537711.57353354 Loss_gradient: [ 24.86053129 -14.54563992] Weight: [ 26464.92055795 281953.90883392]\n",
      "MSE:  9922586760.909876\n"
     ]
    }
   ],
   "source": [
    "# Run Batch Gradient Descent to find optimal Weight vector\n",
    "trained_weight = func.batchGradientDescent(func.mseLoss, func.lossGradient, len(feature), training_data_np, func.linearPhi)\n",
    "\n",
    "# Validate model\n",
    "total_error = 0\n",
    "for i in testing_data_np:\n",
    "    test_x, test_y = i\n",
    "    pred_y = trained_weight.dot(func.linearPhi(test_x))\n",
    "    squared_error = (pred_y - test_y) ** 2\n",
    "    total_error += squared_error\n",
    "mse = 1 / len(testing_data_np) * total_error\n",
    "print(\"MSE: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cross-Validation\n",
    "# TODO: Quadratic Regression\n",
    "# TODO: Periodic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
